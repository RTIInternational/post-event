{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37ffdf4b-5e6f-4c29-8358-a6d371c45c1b",
   "metadata": {},
   "source": [
    "# Load Post-Event Evaluation Data\n",
    "This notebook walks through the steps to load NWM operational forecasts and verifying observations into a cache of parquet files to enable interactive, visual evaluation of the forecasts. The steps of the loading process include the following:\n",
    "\n",
    "1. Start a cluster\n",
    "2. Read the root path and geometry data\n",
    "3. Define the event (dates and region)\n",
    "4. Select datasets to load\n",
    "5. Load the data\n",
    "6. Monitor progress on the cluster dashboard  \n",
    "(Repeat steps 4-6 for each forecast configuration)\n",
    "\n",
    "### First load necessary packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b45faff-11bb-47a1-ada8-a2118cd3750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import dashboard_utils as du\n",
    "import teehr.loading.nwm22.nwm_point_data as tlp\n",
    "import teehr.loading.usgs.usgs as tlu\n",
    "import teehr.loading.nwm22.nwm_grid_data as tlg\n",
    "from dask.distributed import Client\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import panel as pn\n",
    "import geoviews as gv\n",
    "import holoviews as hv\n",
    "import cartopy.crs as ccrs\n",
    "import datetime as dt\n",
    "import time\n",
    "import json\n",
    "import importlib\n",
    "pn.extension()\n",
    "hv.extension('bokeh', logo=False)\n",
    "gv.extension('bokeh', logo=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f339a38-b3fe-49e5-9ce4-0f225b3cea58",
   "metadata": {},
   "source": [
    "## 1. Start a cluster\n",
    "Before loading data, start a cluster of nodes for distributed computing to make the loading faster. The method (GatewayCluster or LocalCluster) depends on whether you are running this notebook locally or in the TEEHR Hub, which is detected automatically below (based on the JupyterHub global username 'jovyan'). The cluster will remain active until you shut it down, so you only need to run this step once. (i.e., no need to rerun to load data for a new event, region or forecast configuration). Note that the cluster does not shut down automatically after a period of inactivity, so it is important to manually shut down the cluster when you are finished loading data (last cell in this notebook). \n",
    "\n",
    "**To monitor data loading progress**:\n",
    "- Click on the Dashboard URL that appears after running the cell below\n",
    "- Got to the dashboard after launching the data loading step further below\n",
    "\n",
    "**If running in TEEHR Hub**, select the number of workers in the GatewayCluster panel:\n",
    "- Select \"Manual Scaling\"\n",
    "- Enter the # of desired workers (initial testing indicates 16 is roughly optimal)\n",
    "- Click \"Scale\" and wait for the # of workers in the left side of the GatewayCluster panel to update\n",
    "\n",
    "*On TEEHR Hub it may take a several minutes for the workers to spin up if the server is inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3022b0c-e812-43b9-82d6-52b922907ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'client' not in locals():\n",
    "    if 'jovyan' in list(Path().absolute().parts):\n",
    "        run_location = 'teehrhub'       \n",
    "        client = Client(n_workers=16)\n",
    "    else:\n",
    "        run_location = 'local'\n",
    "        client = Client()\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b96a5d-67f7-43ae-a61c-b90cbdd718e5",
   "metadata": {},
   "source": [
    "## 2. Read the root path and geometry data\n",
    "For running on the TEEHR Hub, the config file ```teehrhub_config.json``` and is included in the repo. For running locally, only a sample config file (```local_sample.json```) is included in the repo for security purposes (so local paths are not exposed). As described in the README, to work with the notebooks locally, copy ```local_sample.json``` to ```local_config.json``` and edit the path. ```local_config.json``` is included in the .gitignore file to prevent it from being pushed to the repo.\n",
    "\n",
    "Geometry-related data read below include:\n",
    "- ID crosswalks\n",
    "- points/polygons geometry needed to align NWM forecasts with observations\n",
    "- polygons to display in maps to facilitate region selection\n",
    "- grid weights for HUC10 mean areal calculations.\n",
    "\n",
    "Note that HUC10s are currently the default area unit for precipitation processing in this notebook, however grid weights can be externally calculated in TEEHR for any polygon layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c12faab-a1a5-47dd-813f-1b68f91c8248",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'jovyan' in list(Path().absolute().parts):\n",
    "    config_file = 'teehrhub_config.json'\n",
    "else:\n",
    "    config_file = 'local_config.json'\n",
    "\n",
    "# Read the root path in the config file, set geometry subdir name under the root dir\n",
    "root_dir = du.read_root_dir(Path(Path().absolute().parents[0], 'config', config_file))\n",
    "geo_dir = Path(root_dir, 'geo')\n",
    "json_dir = Path(root_dir, 'zarr')\n",
    "event_defs_path = Path(root_dir, 'events', 'event_definitions.json')\n",
    "\n",
    "# read crosswalks\n",
    "usgs_nwm_crosswalk=pd.read_parquet(Path(geo_dir, 'usgs_nwm22_crosswalk.conus.parquet'))\n",
    "usgs_huc12_crosswalk=pd.read_parquet(Path(geo_dir, 'usgs_huc12_crosswalk.conus.parquet'))\n",
    "nwm_huc12_crosswalk=pd.read_parquet(Path(geo_dir, 'nwm22_huc12_crosswalk.conus.parquet'))\n",
    "\n",
    "# read geometry\n",
    "huc2_gdf = gpd.read_parquet(Path(geo_dir, 'huc2_geometry.conus.parquet'))\n",
    "states_gdf = gpd.read_parquet(Path(geo_dir, 'states_geometry.conus.parquet')) \n",
    "usgs_gdf = gpd.read_parquet(Path(geo_dir, 'usgs_point_geometry.conus.parquet'))\n",
    "huc10_gdf = gpd.read_parquet(Path(geo_dir, 'huc10_geometry.conus.parquet'))\n",
    "nwm_gdf = gpd.read_parquet(Path(geo_dir, 'nwm22_centroid_geometry.conus.parquet'))\n",
    "\n",
    "# read huc10 grid weights\n",
    "huc10_grid_weights = pd.read_parquet(Path(geo_dir, 'huc10_grid_weights.conus.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeb767b-7885-488e-8ed7-793fcf5bd065",
   "metadata": {},
   "source": [
    "## 3. Define the event\n",
    "\n",
    "To load and organize the data, the event name, dates, and regional extents are needed.  The event name is used to organize parquet files by event (for faster querying) and for reference in subsequent notebooks. The dates selected are the start and end dates **of the event** (use the same date for a single-day event). The forecast reference times will be identified that include any timesteps overlapping with the event dates.  Those forecasts, and observations corresponding to the value times in those forecasts, will be loaded.  Selecting a subregion (for conus) will also save some time and disk space rather than loading the entire domain. This is particularly true if you are loading forcing data. For simplicity (for now), the widgets below allow you to subset by one or more HUC2 subregions and further subset by lat/lon bounds if desired.\n",
    "\n",
    "Event name and specifications are stored in ```ROOT_DIR/post-event/events/event_definitions.json```\n",
    "\n",
    "### First select whether this is a new or previously defined event, then show/define the event characteristics\n",
    "If you would like to download data again for a previously defined event, select that event name. Otherwise leave the selection as the default \"define new event\" to launch the date/region selection widgets. To change the dates or region of a previously defined event, treat it as a new event and use the same name (it will overwrite the values in event_definitions.json)\n",
    "\n",
    "(TO DO - add other domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c58268-db33-4970-b552-3531d5fb0103",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_events = du.read_event_definitions(event_defs_path)\n",
    "select_event_name = pn.widgets.Select(name='Select new or previously defined event:', options=['define new event'] + list(existing_events.keys()))\n",
    "pn.Row(select_event_name, pn.Column(pn.Spacer(height=30)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9cad0-8a74-4e00-a145-af23ae61ab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "define_event_panel, widgets, selection = du.select_event_widgets(huc2_gdf, states_gdf, existing_events, select_event_name)\n",
    "define_event_panel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ee519-c07b-4cd5-bb0e-59e112a43f59",
   "metadata": {},
   "source": [
    "### Confirm selected features and polygons\n",
    "This step extracts the NWM features within the selected region and forecast reference times overlapping the event date range. A map will appear below with a red boundary around the selected area - confirm that the region and printed forecast reference times match the intended selection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c539b4-9c67-4f18-9780-387c529d9117",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\033[1mSubregion:\\033[0m\")\n",
    "huc2_list = du.get_selected_huc2_list(huc2_gdf, selection.index)\n",
    "if huc2_list == [] and select_event_name != 'define new event':\n",
    "    huc2_list = du.get_existing_event(existing_events, select_event_name)['huc2_list']\n",
    "event_specs = dict(\n",
    "    name = widgets['event_name_input'].value,\n",
    "    start_date = widgets['start_picker'].value,\n",
    "    end_date = widgets['end_picker'].value,\n",
    "    huc2_list = huc2_list,\n",
    "    lat_limits = widgets['lat_slider'].value,\n",
    "    lon_limits = widgets['lon_slider'].value,\n",
    ")\n",
    "huc2_selected = event_specs['huc2_list']\n",
    "latlon_box = du.get_lat_lon_box_from_limits(event_specs['lat_limits'], event_specs['lon_limits'])\n",
    "\n",
    "# get the features and hucs within the selected region\n",
    "usgs_ids = du.get_usgs_id_list_as_str(huc2_selected, latlon_box, usgs_gdf, usgs_huc12_crosswalk)\n",
    "huc10_selected_gdf = du.get_hucs_selected(huc2_selected, latlon_box, huc10_gdf, huc_level = 10)\n",
    "nwm_selected_gdf = du.get_nwm_subset_by_huc10s(huc10_selected_gdf['id'].to_list(), nwm_gdf, nwm_huc12_crosswalk, usgs_ids, usgs_nwm_crosswalk, 'all reaches')\n",
    "print(f\"{len(usgs_ids)} USGS gages selected\\nLoading map...\")\n",
    "outer_bound = du.get_outer_bound(points = nwm_selected_gdf, polys = huc10_selected_gdf)\n",
    "huc2s = gv.Polygons(huc2_gdf, vdims=['huc2'],crs=ccrs.GOOGLE_MERCATOR)\n",
    "states = gv.Polygons(states_gdf, vdims=['STUSPS'], crs=ccrs.GOOGLE_MERCATOR)   \n",
    "\n",
    "button = pn.widgets.Button(name='Update/Store selected event', button_type='primary')\n",
    "def update_event_defs_file(event):\n",
    "    updated_event_defs = du.update_event_definitions(existing_events, event_specs.copy())\n",
    "    du.write_event_definitions(event_defs_path, updated_event_defs)\n",
    "button.on_click(update_event_defs_file)\n",
    "\n",
    "pn.Column(\n",
    "    pn.Row(\n",
    "        pn.Column(pn.Spacer(height=25), button),\n",
    "        states.opts(color_index=None, fill_color='lightgray', nonselection_alpha=1, line_color='white', tools=[''], \n",
    "                    title=f'Selecting reaches within the red boundary') \\\n",
    "        * huc2s.opts(color_index=None, line_color='darkgray', fill_color='none', width=700, height=450) \\\n",
    "        * gv.Polygons(outer_bound, crs=ccrs.GOOGLE_MERCATOR).opts(color_index=None, fill_color='none', line_color = 'red', line_width=2),\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798c5450-5d53-4b9b-a239-794d7c5b7ff4",
   "metadata": {},
   "source": [
    "### Define datasets to load\n",
    "Next define the datasets you want to load for the above defined event and the set of reaches (gaged only or all NWM features).  Note that loading functions search for and load all data available on a given date (if the hour is specified, it is ignored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6383e1-6b7a-4441-b2c8-aaffec092b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(du)\n",
    "define_data_panel, data_widgets = du.select_data_widgets()\n",
    "date_strings = du.list_nwm_dates_for_event_dates(widgets['start_picker'].value, widgets['end_picker'].value)\n",
    "for key in date_strings.keys():\n",
    "    print(date_strings[key])\n",
    "pn.Column(pn.Spacer(height=10), define_data_panel, pn.Spacer(height=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91526ad-6bbb-42a9-8fdd-83a9a760d6f9",
   "metadata": {},
   "source": [
    "## Verify dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6b6f81-dbc2-43c1-819e-91b617f42ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "reach_set = data_widgets['select_reach_set'].value\n",
    "nwm_selected_gdf = du.get_nwm_subset_by_huc10s(huc10_selected_gdf['id'].to_list(), nwm_gdf, nwm_huc12_crosswalk, usgs_ids, usgs_nwm_crosswalk, reach_set)\n",
    "nwm_ids = du.get_nwm_id_list_as_int(nwm_selected_gdf, nwm_huc12_crosswalk)\n",
    "\n",
    "forecast_configuration = data_widgets['select_forecast_config'].value[0]\n",
    "ref_start = data_widgets['select_ref_start'].value\n",
    "ref_end = data_widgets['select_ref_end'].value\n",
    "adj_ref_end = du.validate_dates(ref_start, ref_end, forecast_configuration, \"reference\")\n",
    "ref_n_days = (adj_ref_end - ref_start).days + 1\n",
    "val_start = data_widgets['select_value_start'].value\n",
    "val_end = data_widgets['select_value_end'].value\n",
    "adj_val_end = du.validate_dates(val_start, val_end, \"usgs\", \"valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de5e87e-9913-458e-9b14-139f3513a603",
   "metadata": {},
   "source": [
    "# Load streamflow data\n",
    "Load the streamflow data for the forecast configuration, data sources (forecast and/or observed) and reach set defined above. If selecting 'all reaches', USGS data will be loaded as observations for gaged reaches and NWM extended analysis/assim data will be loaded for ungaged reaches to serve as a proxy for 'observed'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef63be-779f-41be-90f2-689975e12d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dir = Path(root_dir, 'events', event_specs['name'], 'timeseries')\n",
    "\n",
    "variable_name = 'streamflow'\n",
    "output_type = 'channel_rt'\n",
    "\n",
    "forecast_configuration = data_widgets['select_forecast_config'].value[0]\n",
    "if forecast_configuration == 'medium_range_mem1':\n",
    "    output_type = 'channel_rt_1'\n",
    "\n",
    "if forecast_configuration != 'none':\n",
    "    t_start = time.time()\n",
    "    ts_dir_config = Path(ts_dir, forecast_configuration)\n",
    "    json_dir_config = Path(json_dir, forecast_configuration)\n",
    "    print(f\"Loading {forecast_configuration} {variable_name} from {ref_start} to {ref_end}\")\n",
    "    tlp.nwm_to_parquet(\n",
    "        forecast_configuration,\n",
    "        output_type,\n",
    "        variable_name,\n",
    "        ref_start,\n",
    "        ref_n_days,\n",
    "        nwm_ids,\n",
    "        json_dir_config,\n",
    "        ts_dir_config\n",
    "    )\n",
    "    print(f\"...{forecast_configuration} {variable_name} loading complete in {(time.time() - t_start)/60} minutes\\n\")       \n",
    "\n",
    "# get corresponding observed data if requested\n",
    "adj_val_end = du.validate_dates(val_start, val_end, \"usgs\", \"valid\")\n",
    "if 'USGS*' in data_widgets['select_observed_source'].value:\n",
    "    t_start = time.time()\n",
    "    ts_dir_config = Path(ts_dir, 'usgs')\n",
    "    print(f\"Loading USGS {variable_name} from {val_start} to {val_end}\") \n",
    "    tlu.usgs_to_parquet(\n",
    "        usgs_ids,\n",
    "        dt.datetime.combine(val_start, dt.time(hour=0)),\n",
    "        dt.datetime.combine(val_end, dt.time(hour=23)),\n",
    "        ts_dir_config,\n",
    "        chunk_by = 'day'\n",
    "    )\n",
    "    print(f\"...USGS loading complete in {(time.time() - t_start)/60} minutes\\n\") \n",
    "\n",
    "ana_list = ['analysis_assim_extend', 'analysis_assim', 'analysis_assim_extend_no_da', 'analysis_assim_no_da']\n",
    "for ana in ana_list:\n",
    "    adj_val_end = du.adj_valtime_end(val_end, ana).date()\n",
    "    val_n_days = (adj_val_end - val_start).days + 1\n",
    "    if ana in data_widgets['select_observed_source'].value:\n",
    "        t_start = time.time()\n",
    "        ts_dir_config = Path(ts_dir, ana)\n",
    "        json_dir_config = Path(json_dir, ana)\n",
    "        if 'extend' in ana:\n",
    "            tm_range = range(0,28)\n",
    "        else:\n",
    "            tm_range = range(0,2)   \n",
    "\n",
    "        print(f\"Loading {ana} {variable_name} from {val_start} to {adj_val_end}\")            \n",
    "        tlp.nwm_to_parquet(\n",
    "            ana,\n",
    "            output_type,\n",
    "            variable_name,\n",
    "            val_start,\n",
    "            val_n_days,        \n",
    "            nwm_ids,\n",
    "            json_dir_config,\n",
    "            ts_dir_config,\n",
    "            tm_range\n",
    "        )\n",
    "        print(f\"...{ana} {variable_name} loading complete in {(time.time() - t_start)/60} minutes\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995a00b3-d621-4392-ad07-a9016a1a4d48",
   "metadata": {},
   "source": [
    "# Load mean areal precipitation data\n",
    "\n",
    "Now (if desired) calculate and load mean areal precipitation data for the forecast configuration and data sources (forecast and/or observed). NWM extended analysis/assim forcing data (StageIV) will be loaded as the best proxy for 'observed'.  Note that HUC10s are currently the default area unit for precipitation processing in this notebook, however grid weights can be externally calculated in TEEHR for any polygon layer.me period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5d7bff-97a5-45a1-a8f9-8ca21f6463c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_dir = Path(root_dir, 'events', event_specs['name'], 'timeseries')\n",
    "\n",
    "forecast_configuration = data_widgets['select_forecast_config'].value[0]\n",
    "forcing_forecast_configuration = 'forcing_' + forecast_configuration\n",
    "if forcing_forecast_configuration == 'forcing_medium_range_mem1':\n",
    "    forcing_forecast_configuration = 'forcing_medium_range'\n",
    "variable_name = 'RAINRATE'\n",
    "output_type = 'forcing'\n",
    "json_dir_config = Path(json_dir, forcing_forecast_configuration)\n",
    "ts_dir_config = Path(ts_dir, forcing_forecast_configuration)\n",
    "\n",
    "# write subset of weights to temporary file (necessary to avoid memory issues when passing in memory for distributed computing)\n",
    "grid_weights_subset = huc10_grid_weights[huc10_grid_weights['zone'].isin(huc10_selected_gdf['id'])]\n",
    "grid_weights_subset.to_parquet(Path(geo_dir, 'temp_grid_weights_subset.parquet'))\n",
    "\n",
    "if forecast_configuration != 'none':\n",
    "    t_start = time.time()\n",
    "    json_dir_config = Path(json_dir, forcing_forecast_configuration)\n",
    "    ts_dir_config = Path(ts_dir, forcing_forecast_configuration)\n",
    "    print(f\"Loading {forcing_forecast_configuration} {variable_name} from {ref_start} to {ref_end}\")\n",
    "    tlg.nwm_grids_to_parquet(\n",
    "        forcing_forecast_configuration,\n",
    "        output_type,\n",
    "        variable_name,\n",
    "        ref_start,\n",
    "        ref_n_days,\n",
    "        Path(geo_dir, 'temp_grid_weights_subset.parquet'),\n",
    "        json_dir_config,\n",
    "        ts_dir_config\n",
    "    )\n",
    "    print(f\"...{forcing_forecast_configuration} {variable_name} loading complete in {(time.time() - t_start)/60} minutes\\n\")\n",
    "\n",
    "ana_list = ['analysis_assim_extend', 'analysis_assim']\n",
    "for ana in list(set(ana_list) & set(data_widgets['select_observed_source'].value)):\n",
    "    adj_val_end = du.adj_valtime_end(val_end, ana).date()\n",
    "    val_n_days = (adj_val_end - val_start).days + 1\n",
    "    if ana in data_widgets['select_observed_source'].value:\n",
    "        t_start = time.time()\n",
    "        forcing_ana_config = 'forcing_' + ana\n",
    "        ts_dir_config = Path(ts_dir, forcing_ana_config)\n",
    "        json_dir_config = Path(json_dir, forcing_ana_config)\n",
    "        print(f\"Loading {forcing_ana_config} {variable_name} from {val_start} to {adj_val_end}\") \n",
    "        if 'extend' in forcing_ana_config:\n",
    "            tm_range = range(0,28)\n",
    "        else:\n",
    "            tm_range = [2]          \n",
    "        tlg.nwm_grids_to_parquet(\n",
    "            forcing_ana_config,\n",
    "            output_type,\n",
    "            variable_name,\n",
    "            val_start,\n",
    "            val_n_days, \n",
    "            Path(geo_dir, 'temp_grid_weights_subset.parquet'),\n",
    "            json_dir_config,\n",
    "            ts_dir_config,\n",
    "            tm_range\n",
    "        )\n",
    "        print(f\"...{forcing_ana_config} {variable_name} loading complete in {(time.time() - t_start)/60} minutes\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe88cac-ccbd-482a-970e-b37d3d9868db",
   "metadata": {},
   "outputs": [],
   "source": [
    "forcing_ana_config = 'forcing_analysis_assim'\n",
    "ts_dir_config = Path(ts_dir, forcing_ana_config)\n",
    "json_dir_config = Path(json_dir, forcing_ana_config)\n",
    "variable_name = 'RAINRATE'\n",
    "output_type = 'forcing'\n",
    "start = dt.datetime(2023, 8, 21)\n",
    "n_days = 1\n",
    "      \n",
    "tlg.nwm_grids_to_parquet(\n",
    "    forcing_ana_config,\n",
    "    output_type,\n",
    "    variable_name,\n",
    "    start,\n",
    "    n_days,\n",
    "    Path(geo_dir, 'temp_grid_weights_subset.parquet'),\n",
    "    json_dir_config,\n",
    "    ts_dir_config,\n",
    "    range(0,2)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efec0b6c-b2c9-41c5-b409-69cd73b9fe83",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
